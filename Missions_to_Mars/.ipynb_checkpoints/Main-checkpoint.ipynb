{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 85.0.4183\n",
      "[WDM] - Get LATEST driver version for 85.0.4183\n",
      "[WDM] - Driver [C:\\Users\\tawnyn\\.wdm\\drivers\\chromedriver\\win32\\85.0.4183.87\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "executable_path = {\"executable_path\": ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url for browser\n",
    "news_domain = 'https://mars.nasa.gov'\n",
    "url = news_domain + '/news'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fb39120fd381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_html\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Retrieve all elements that contain article info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'slide'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Iterate through each news article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Loop to get news_title and news_p\n",
    "for x in range(1):\n",
    "    # HTML object\n",
    "    html = browser.html\n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Retrieve all elements that contain article info\n",
    "    articles = soup.find_all('li', class_='slide')[x]\n",
    "\n",
    "    # Iterate through each news article\n",
    "    for article in articles:\n",
    "        # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "        news_title  = article.find('div', class_=\"content_title\").text\n",
    "        news_p = article.find('div', class_='article_teaser_body').text\n",
    "        img_header =article.find('div', class_='list_image')\n",
    "        img = img_header.find('img')['src']\n",
    "        news_image = 'https://mars.nasa.gov' + img\n",
    "        print(news_title)\n",
    "        print(news_p)\n",
    "        print(news_image)\n",
    "\n",
    "    # Click the 'More' button on each page\n",
    "    try:\n",
    "        browser.click_link_by_text('More')\n",
    "        print(\"Mars News Complete\")\n",
    "          \n",
    "    except:\n",
    "        print(\"Mars News Scraping Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url for browser\n",
    "domain = 'https://www.jpl.nasa.gov'\n",
    "space_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(space_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA09320_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all elements that contain image link\n",
    "images = soup.find_all('article')\n",
    "\n",
    "for img in images:\n",
    "    data_link = img.find('a')['data-link']\n",
    "    \n",
    "    # Open data link to get full size image\n",
    "    data_link_url = domain + data_link\n",
    "    \n",
    "\n",
    "    try:\n",
    "        browser.visit(data_link_url)\n",
    "        html = browser.html\n",
    "\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Make sure to find the image url to the full size .jpg image.        \n",
    "        iamge_articles = soup.find_all('article')\n",
    "        \n",
    "        for iamge_article in iamge_articles:\n",
    "            figure = iamge_article.find('figure')\n",
    "            figure_url = figure.find('a')['href']\n",
    "            \n",
    "             # Make sure to save a complete url string for this image.\n",
    "            featured_image_url  = domain + figure_url\n",
    "            print(featured_image_url)        \n",
    "        \n",
    "    except:\n",
    "        print(\"Featured Image Scraping Complete\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage here \n",
    "mars_url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equatorial Diameter:</td>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polar Diameter:</td>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mass:</td>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moons:</td>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orbit Distance:</td>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Topic                          Value\n",
       "0  Equatorial Diameter:                       6,792 km\n",
       "1       Polar Diameter:                       6,752 km\n",
       "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
       "3                Moons:            2 (Phobos & Deimos)\n",
       "4       Orbit Distance:       227,943,824 km (1.38 AU)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "tables = pd.read_html(mars_url)\n",
    "\n",
    "df = tables[0]\n",
    "df.columns = ['Topic', 'Value']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n",
    "usgs_site = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(usgs_site)\n",
    "\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all elements that contain image section\n",
    "image_section = soup.find_all('div', class_='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg\n",
      "Valles Marineris Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg\n"
     ]
    }
   ],
   "source": [
    "# You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "# Create lists to hold each title and image\n",
    "titles = []\n",
    "images = []\n",
    "\n",
    "# loop to append title and images to lists\n",
    "for image in image_section:\n",
    "    figure_url = image.find('a')['href']\n",
    "    image_url = 'https://astrogeology.usgs.gov' + figure_url\n",
    "    title = image.find('h3').text\n",
    "    print(title)\n",
    "#     print(image_url)\n",
    "    \n",
    "    titles.append(title)\n",
    "    \n",
    "    try:\n",
    "        browser.visit(image_url)\n",
    "        html = browser.html\n",
    "\n",
    "        # Parse HTML with Beautiful Soup\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Retrieve all elements that contain image section\n",
    "        thumnb_url = soup.find_all('div', class_='downloads')\n",
    "\n",
    "        for thumb in thumnb_url:\n",
    "            image_list = thumb.find('li')\n",
    "            image_full_url = image_list.find('a')['href']\n",
    "            images.append(image_full_url)\n",
    "            print(image_full_url)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print('Did not work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. \n",
    "# Use a Python dictionary to store the data using the keys img_url and title.\n",
    "image_urls = {}\n",
    "image_urls = dict(zip(titles, images))\n",
    "\n",
    "# Append the dictionary with the image url string and the hemisphere title to a list. \n",
    "# This list will contain one dictionary for each hemisphere.\n",
    "hemisphere_image_urls = [] \n",
    "for key, value in image_urls.items(): \n",
    "    hemisphere_image_urls.append({'title' : key, 'img_url' : value}) \n",
    "    \n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_title': \"NASA's Perseverance Mars Rover Gets Balanced\",\n",
       " 'news_p': \"The mission team performed a crucial weight-balancing test on the rover in preparation for this summer's history-making launch to the Red Planet.\",\n",
       " 'featured_image': 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA18906_hires.jpg'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_data = {\n",
    "    \"news_title\": news_title,\n",
    "    \"news_p\": news_p,\n",
    "    \"featured_image\" : featured_image_url\n",
    "} \n",
    "mars_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
